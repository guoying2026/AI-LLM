
# AI大模型历史发展进程

## 1、方法论

### 1.1、连接主义 Connectionism

#### a、定义

> 符号主义，也称为“逻辑主义”，是AI早期的主要研究方法。
> 
> 
> 它以人类的逻辑思维为基础，试图通过显式的符号运算和逻辑推理，模拟人类的思考过程。
> 
> 
> 符号主义的方法强调知识表示和推理，常常使用显式的规则
> 
> 
> （ 例如，if-then规则）来表示知识。符号主义的典型例子包括专家系统、逻辑编程等。
#### b、历史

> 连接主义的历史可以追溯到 **神经网络** 早期的发展
>
> 
> 早在1940年代，Warren McCulloch和Walter Pitts就提出了最早的神经网络模型。然而。这个领域在接下来的几十年里发展缓慢，
>
> 
> 直到1980年代，由于计算机硬件的发展和反向传播算法的实现，这个领域开始复苏。这被称为
> “神经网络的二次繁荣“。
> 
> 
> 从此以后，神经网络在许多任务中都取得了显著的成功。代表产品有
> 深度学习框架“tensorFlow”和"PyTorch",深度学习模型如AlexNet, ResNet, BERT, GPT等。

### 1.2、符号主义 Symbolism

#### a、定义

> 连接主义又称神经网络方法，它的基础是模拟生物神经网络的工作机制，
> 
> 
> 通过训练神经网络模型来学习从输入到输出的映射关系，而不是通过显式的规则进行推理。
> 
> 
> 连接主义方法强调的是学习和适应，其中的知识通常以网络权重的形式隐式地存储在模型中，
> 
> 
> 而不是以显式的规则形式存在。深度学习就是连接主义的一个重要代表。

#### b、历史

> 符号主义的历史可以追溯到 **人工智能** 早期的发展，符号主义在一定程度上定义了人工智能的早期。
> 
> 
> 早在1950年代和1960年代，人们就开始研究如何让计算机执行逻辑推理。
> 
> 
> 1970年代和1980年代，专家系统开始流行，它们使用一组预先编码的规则来模拟人类专家的决策过程。
> 
> 
> 到了1990年代，人们对这种方法的热情开始减弱，原因在于这种方法的扩展性有限，很难处理复杂的、真实世界的问题。代表产品有专家系统（如MYCIN）、逻辑编程语言（如Prolog）等。

### 1.3 符号主义和连接主义结合

#### b、历史

> 这两种理论在历史上经历了起起落落，但是近年来，人们开始尝试将这两种理论结合起来，以便结合它们的优点。
> 
>
> 例如，神经符号系统试图将神经网络的能力（如处理模糊数据、学习从数据中学习）和符号系统的优点（如明确的逻辑推理能力）结合起来。
 
## 2、
## 早期机器翻译
### 翻译架构(Encoder-Decoder)
在机器翻译中，最常用的机器学习架构是所谓的 **"编码器-解码器"（Encoder-Decoder）架构**
> 这个架构中，一个模型（编码器 encoder）会把源语言文本转化为一个中间表示，
> 然后另一个模型（解码器 decoder）会把这个中间表示转换为目标语言成本。

### 

## 3、数学基础

### 3.1、线性代数

#### a、意义

> 线性代数提供的向量和矩阵，提供了一种看待世界的抽象视角：
> 
> 
> 万事万物都可以被抽象成某些特征的组合，并在由预制规则定义的框架之下以静态和动态的方式加以观察。
> 
> 
> 线性代数是用虚拟数字世界表示真实物理世界的工具。

#### b、名词

- 集合
- 标量
- 向量
- 矩阵
- 张量
- 范数
- L1范数
- L2范数
- L无穷 范数
- 内积
- 正交
- 线性空间
- 内积空间
- 正交基
- 标准正交基
- 矩阵
- 特征值
- 特征向量

### 3.2、概率论

- 频率定量
  - 古典概率
  - 条件概率
    - 联合概率
    - 全概率公式
    - 逆概率
    - 贝叶斯公式
    - 贝叶斯定理
      - 先验概率
      - 似然概率
      - 后验概率
      

- 概率估计
  - 最大似然估计法 maximum likelihood estimation
  - 最大后验概率法 maximum a posteriori estimation


- 随机变量
  - 离散型随机变量
    - 概率质量函数 probability mass function 
  - 连续型随机变量
    - 概率密度函数 probability density function


- 离散分布
  - 两点分布 Bernoulli distribution
  - 二项分布 Binomial distribution
  - 泊松分布 Poisson distribution

  
- 连续分布
  - 均匀分布 uniform distribution
  - 指数分布 exponential distribution
  - 正态分布 normal distribution
  

- 数字特征
    - 数学期望expected value
    - 方差  variance
    - 协方差 covariance

### 3.3、数理统计 mathematical statistics

- 样本 sample


- 总体 population


- 小概率事件


- 统计量的样本的函数 
    - 样本均值
    - 样本方差


- 统计推断
    - 参数估计 estimation theory
    - 假设检验 hypothesis test


- 参数估计
    - 点估计 point estimation
        - 矩估计法 method of moments
            - 矩 
            - k阶矩
        - 最大似然估计法 maximum likelihood estimation
            - 似然函数
                - 若干概率质量函数 / 概率密度函数 相乘，进一步转换成对数方程
                - 似然函数的取值最大化==微积分中求解函数最大值
        - 估计量评价
            - 无偏性
            - 有效性
            - 一致性
    - 区间估计 interval estimation
        - 置信区间 confidence interval
        - 置信上限
        - 置信下限
        - 置信水平


- 假设检验
  - 推断
      - 原假设
      - 备择假设
      - 检验过程
      - 泛化能力
      - 错误率
          - 泛化错误率
          - 测试错误率
  - 对泛化性能的解释
      - 泛化误差
        - 偏差 bias
          - 模型的欠拟合性
        - 方差 variance
          - 模型的过拟合性 
        - 噪声 noise
          - 任务本身难度 

### 3.4、最优化方法

> 人工智能的目标就是最优化；
> 
> 
> 在复杂环境与多体交互中做出最优决策；
> 
> 
> 几乎所有的人工智能问题最后都会归结为一个优化问题的求解；

- 最优化理论 optimization


- 目标函数 objective function / 评价函数


- 最优化问题
    - 无约束优化 unconstrained optimization
      - 梯度下降法  gradient descent
        - 梯度方向：目标函数导数的反方向
    - 约束优化 constrained optimization
      - 线性规划 
    
    > 约束优化问题通常比无约束优化问题更加复杂；
    >
    >
    >  但通过拉格朗日乘子（Lagrange multiplier）的引入可以将含有 n 个变量和 k 个约束条件的问题转化为含有 (n+k) 个变量的无约束优化问题。
    > 
    

- 最优化算法
    - 目标函数的全局最小值 global minimum
        - 全局最小值 
    - 目标函数的局部极小值 local minimum

### 3.5、信息论

### 3.6、形式逻辑