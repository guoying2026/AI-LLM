
# AI大模型历史发展进程

## 1、方法论

### 1.1、连接主义 Connectionism

#### a、定义

> 符号主义，也称为“逻辑主义”，是AI早期的主要研究方法。
> 
> 
> 它以人类的逻辑思维为基础，试图通过显式的符号运算和逻辑推理，模拟人类的思考过程。
> 
> 
> 符号主义的方法强调知识表示和推理，常常使用显式的规则
> 
> 
> （ 例如，if-then规则）来表示知识。符号主义的典型例子包括专家系统、逻辑编程等。
#### b、历史

> 连接主义的历史可以追溯到 **神经网络** 早期的发展
>
> 
> 早在1940年代，Warren McCulloch和Walter Pitts就提出了最早的神经网络模型。然而。这个领域在接下来的几十年里发展缓慢，
>
> 
> 直到1980年代，由于计算机硬件的发展和反向传播算法的实现，这个领域开始复苏。这被称为
> “神经网络的二次繁荣“。
> 
> 
> 从此以后，神经网络在许多任务中都取得了显著的成功。代表产品有
> 深度学习框架“tensorFlow”和"PyTorch",深度学习模型如AlexNet, ResNet, BERT, GPT等。

### 1.2、符号主义 Symbolism

#### a、定义

> 连接主义又称神经网络方法，它的基础是模拟生物神经网络的工作机制，
> 
> 
> 通过训练神经网络模型来学习从输入到输出的映射关系，而不是通过显式的规则进行推理。
> 
> 
> 连接主义方法强调的是学习和适应，其中的知识通常以网络权重的形式隐式地存储在模型中，
> 
> 
> 而不是以显式的规则形式存在。深度学习就是连接主义的一个重要代表。

#### b、历史

> 符号主义的历史可以追溯到 **人工智能** 早期的发展，符号主义在一定程度上定义了人工智能的早期。
> 
> 
> 早在1950年代和1960年代，人们就开始研究如何让计算机执行逻辑推理。
> 
> 
> 1970年代和1980年代，专家系统开始流行，它们使用一组预先编码的规则来模拟人类专家的决策过程。
> 
> 
> 到了1990年代，人们对这种方法的热情开始减弱，原因在于这种方法的扩展性有限，很难处理复杂的、真实世界的问题。代表产品有专家系统（如MYCIN）、逻辑编程语言（如Prolog）等。

### 1.3 符号主义和连接主义结合

#### b、历史

> 这两种理论在历史上经历了起起落落，但是近年来，人们开始尝试将这两种理论结合起来，以便结合它们的优点。
> 
>
> 例如，神经符号系统试图将神经网络的能力（如处理模糊数据、学习从数据中学习）和符号系统的优点（如明确的逻辑推理能力）结合起来。
 
## 2、
## 早期机器翻译
### 翻译架构(Encoder-Decoder)
在机器翻译中，最常用的机器学习架构是所谓的 **"编码器-解码器"（Encoder-Decoder）架构**
> 这个架构中，一个模型（编码器 encoder）会把源语言文本转化为一个中间表示，
> 然后另一个模型（解码器 decoder）会把这个中间表示转换为目标语言成本。

### 

## 3、数学基础

### 3.1、线性代数

#### a、意义

> 线性代数提供的向量和矩阵，提供了一种看待世界的抽象视角：
> 
> 
> 万事万物都可以被抽象成某些特征的组合，并在由预制规则定义的框架之下以静态和动态的方式加以观察。
> 
> 
> 线性代数是用虚拟数字世界表示真实物理世界的工具。

#### b、名词

- 集合
- 标量
- 向量
- 矩阵
- 张量
- 范数
- L1范数
- L2范数
- L无穷 范数
- 内积
- 正交
- 线性空间
- 内积空间
- 正交基
- 标准正交基
- 矩阵
- 特征值
- 特征向量

### 3.2、概率论

- 频率定量
  - 古典概率
  - 条件概率
    - 联合概率
    - 全概率公式
    - 逆概率
    - 贝叶斯公式
    - 贝叶斯定理
      - 先验概率
      - 似然概率
      - 后验概率
      

- 概率估计
  - 最大似然估计法 maximum likelihood estimation
  - 最大后验概率法 maximum a posteriori estimation


- 随机变量
  - 离散型随机变量
    - 概率质量函数 probability mass function 
  - 连续型随机变量
    - 概率密度函数 probability density function


- 离散分布
  - 两点分布 Bernoulli distribution
  - 二项分布 Binomial distribution
  - 泊松分布 Poisson distribution

  
- 连续分布
  - 均匀分布 uniform distribution
  - 指数分布 exponential distribution
  - 正态分布 normal distribution
  

- 数字特征
    - 数学期望expected value
    - 方差  variance
    - 协方差 covariance

### 3.3、数理统计 mathematical statistics

- 样本 sample


- 总体 population


- 小概率事件


- 统计量的样本的函数 
    - 样本均值
    - 样本方差


- 统计推断
    - 参数估计 estimation theory
    - 假设检验 hypothesis test


- 参数估计
    - 点估计 point estimation
        - 矩估计法 method of moments
            - 矩 
            - k阶矩
        - 最大似然估计法 maximum likelihood estimation
            - 似然函数
                - 若干概率质量函数 / 概率密度函数 相乘，进一步转换成对数方程
                - 似然函数的取值最大化==微积分中求解函数最大值
        - 估计量评价
            - 无偏性
            - 有效性
            - 一致性
    - 区间估计 interval estimation
        - 置信区间 confidence interval
        - 置信上限
        - 置信下限
        - 置信水平


- 假设检验
  - 推断
      - 原假设
      - 备择假设
      - 检验过程
      - 泛化能力
      - 错误率
          - 泛化错误率
          - 测试错误率
  - 对泛化性能的解释
      - 泛化误差
        - 偏差 bias
          - 模型的欠拟合性
        - 方差 variance
          - 模型的过拟合性 
        - 噪声 noise
          - 任务本身难度 

### 3.4、最优化方法

> 人工智能的目标就是最优化；
> 
> 
> 在复杂环境与多体交互中做出最优决策；
> 
> 
> 几乎所有的人工智能问题最后都会归结为一个优化问题的求解；

- 最优化理论 optimization


- 目标函数 objective function / 评价函数


- 最优化问题


- 最优化算法
    - 目标函数的全局最小值 global minimum
        - 全局最小值 
    - 目标函数的局部极小值 local minimum


- 传统基于数学理论的最优化方法
    - 无约束优化 unconstrained optimization
        - 线性搜索方法 line search
            > 其寻找最小值点的基本思想都是先确定方向，再确定步长；所以把梯度下降法和牛顿法统称为线性搜索方法。
            - 梯度下降法  gradient descent
                - 梯度方向：目标函数导数的反方向
                > 当函数的输入为向量时，目标函数的图象就变成了高维空间上的曲面;
                    这时的梯度就是垂直于曲面等高线并指向高度增加方向的向量; 
                    也就携带了高维空间中关于方向的信息;
                    而要让目标函数以最快的速度下降，
                    就需要让自变量在负梯度的方向上移动。
                >
                > 这个结论翻译成数学语言就是“多元函数沿其负梯度方向下降最快”，
                    这也是梯度下降法的理论依据。
    
                - 单个样本的梯度下降法
                - 多个样本的梯度下降法
                    - 批处理模式 batch processing
                    - 随机梯度下降法 stochastic gradient descent
                - 使用    目标函数的一阶导数 first-order derivative
                > 目标函数如何随输入的变化而变化 
                - 没有使用 目标函数的二阶导数 second-order derivative
                > 一阶导数如何随输入的变化而变化，提供了关于目标函数曲率（curvature）的信息。
                - 曲率 curvature
                > 曲率影响的是目标函数的下降速度。
                    - 曲率为正
                    > 目标函数会比梯度下降法的预期下降得更慢;
                    - 曲率为负
                    > 目标函数则会比梯度下降法的预期下降的更快;
            > 梯度下降法不能利用二阶导数包含的曲率信息，只能利用目标函数的局部性质，因而难免盲目的搜索中。
            >
            > 已知目标函数可能在多个方向上都具有增加的导数，意味着下降的梯度具有多种选择。但不同的选择的效果显然有好有坏。
            >
            > 遗憾的是，梯度下降法无法获知关于导数的变化信息，也就不知道应该探索导数长期为负的方向。 
            >
            > 由于不具备观察目标函数的全局视角，在使用中梯度下降法就会走出一些弯路，导致收敛速度变慢。
            >
            > 而二阶导数所包含的全局信息能够为梯度下降的方向提供指导，进而获得更优的收敛性。
            - 牛顿法
                > 将二阶导数引入优化过程
                >
                > 目标函数首先被泰勒展开，写成二阶近似的形式（相比之下，梯度下降法只保留了目标函数的一阶近似）
                >
                > 此时再对二阶近似后的目标函数求导，并令其导数等于 0，得到的向量表示的就是下降最快的方向。相比于梯度下降法，牛顿法的收敛速度更快。
        - 置信域方法
            > 其寻找最小值点的基本思路是先确定步长，以步长为参数划定一个区域，再在这个区域内寻找最快下降的方向。
    - 约束优化 constrained optimization
        - 线性规划 
    
        > 约束优化问题通常比无约束优化问题更加复杂；
        >
        >
        >  但通过拉格朗日乘子（Lagrange multiplier）的引入可以将含有 n 个变量和 k 个约束条件的问题转化为含有 (n+k) 个变量的无约束优化问题。
        > 


- 启发式算法
    > 启发式算法的灵感来源于 20 世纪 50 年代诞生的仿生学，它将生物进化等自然现象的机理应用于现实世界复杂问题的优化之中，并取得了不俗的效果。
    >
    > 相对于传统的基于数学理论的最优化方法，启发式算法显得返璞归真。启发式算法的核心思想就是大自然中 " 优胜劣汰 " 的生存法则，并在算法的实现中添加了选择和突变等经验因素。
    > 
    >  事实上，搜索越多并不意味着智能越高，智能高的表现恰恰是能够善用启发式策略，不用经过大量搜索也能解决问题。
    - 模拟生物进化的遗传算法 genetic algorithm
    - 模拟统计物理中固体结晶过程的模拟退火算法 simulated annealing
    - 模拟低等动物产生集群智能的蚁群算法 ant colony optimization
    - 神经网络：模拟的则是大脑中神经元竞争和协作的机制。

### 3.5、信息论

> 信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁。
> 

- 熵 
    > 熵”这个词来源于另一位百科全书式的科学家约翰·冯诺伊曼，他的理由是没人知道熵到底是什么。
    虽然这一概念已经在热力学中得到了广泛使用，
    但直到引申到信息论后，熵的本质才被解释清楚，即一个系统内在的混乱程度
    > 

- 自信息量


- 信源


- 信息熵


- 条件熵


- 互信息


- 信息增益


- 信息增益比


- Kullback-Leibler 散度
### 3.6、形式逻辑


## 机器学习

## 人工神经网络

## 深度学习

## 深度学习框架下的神经网络

## 深度学习之外的人工智能

